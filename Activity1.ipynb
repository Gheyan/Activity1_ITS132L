{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead33ee9-64f0-4864-8cec-8379c3fc82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900cb891-aafa-49a6-b3e7-e08afed5c940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SensorID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>SoilMoisture</th>\n",
       "      <th>pH</th>\n",
       "      <th>Fertilizer</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s16</td>\n",
       "      <td>12-Mar-25</td>\n",
       "      <td>15.9</td>\n",
       "      <td>25</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>n field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s36</td>\n",
       "      <td>4/1/2025</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>South Field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s59</td>\n",
       "      <td>20-Mar-25</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>EASTFIELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S009</td>\n",
       "      <td>3/31/2025</td>\n",
       "      <td>14</td>\n",
       "      <td>55</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>West Field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S051</td>\n",
       "      <td>9/4/2025</td>\n",
       "      <td>17.1</td>\n",
       "      <td>56</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>W field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10705</th>\n",
       "      <td>S052</td>\n",
       "      <td>8/4/2025</td>\n",
       "      <td>25.9</td>\n",
       "      <td>48</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>W field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>S060</td>\n",
       "      <td>3/24/2025</td>\n",
       "      <td>30.4</td>\n",
       "      <td>22</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Nrth Field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10707</th>\n",
       "      <td>s08</td>\n",
       "      <td>28-Mar-25</td>\n",
       "      <td>37.8</td>\n",
       "      <td>27</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>westField</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10708</th>\n",
       "      <td>S30</td>\n",
       "      <td>4/5/2025</td>\n",
       "      <td>27.4</td>\n",
       "      <td>46</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3.6</td>\n",
       "      <td>north field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10709</th>\n",
       "      <td>S031</td>\n",
       "      <td>6-Mar-25</td>\n",
       "      <td>19.6</td>\n",
       "      <td>56</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>e field</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10710 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SensorID       Date Temperature SoilMoisture   pH Fertilizer  \\\n",
       "0          s16  12-Mar-25        15.9           25  7.8        1.4   \n",
       "1          s36   4/1/2025          25           50  6.9        1.7   \n",
       "2          s59  20-Mar-25          32           54  6.9        1.6   \n",
       "3         S009  3/31/2025          14           55  5.9        3.7   \n",
       "4         S051   9/4/2025        17.1           56  7.4        4.2   \n",
       "...        ...        ...         ...          ...  ...        ...   \n",
       "10705     S052   8/4/2025        25.9           48  7.4        2.5   \n",
       "10706     S060  3/24/2025        30.4           22  6.1        2.1   \n",
       "10707      s08  28-Mar-25        37.8           27  6.6        2.2   \n",
       "10708      S30   4/5/2025        27.4           46  6.3        3.6   \n",
       "10709     S031   6-Mar-25        19.6           56  6.2        3.8   \n",
       "\n",
       "          Location  \n",
       "0          n field  \n",
       "1      South Field  \n",
       "2        EASTFIELD  \n",
       "3       West Field  \n",
       "4          W field  \n",
       "...            ...  \n",
       "10705      W field  \n",
       "10706   Nrth Field  \n",
       "10707    westField  \n",
       "10708  north field  \n",
       "10709      e field  \n",
       "\n",
       "[10710 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"smart_farm_raw(in).csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5ecfc-7905-4919-814f-8bbdbba8bd55",
   "metadata": {},
   "source": [
    "## 1. List at least five specific data quality issues present in this dataset. For each, reference the classification (e.g., missing, noisy, inconsistent) discussed in the slides.\n",
    "\n",
    "The following below are the list of data quality issues that are present (refer to the codes below this cell for further explanation).\n",
    "- 1. Incomplete or null values are present such as nan, and missing.\n",
    "- 2. Inconsistencies are present such as id formating, and date formats.\n",
    "- 3. Noisy values are present such as negative values being present within ph levels that are commonly within the range of 0-14.\n",
    "- 4. Duplicate records are present within the dataset.\n",
    "- 5. Data type issues are also present wherein some columns have a mix of datatypes such as integers and floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760473e5-c14c-436c-ba54-f611fba49ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SensorID          0\n",
       "Date              0\n",
       "Temperature     408\n",
       "SoilMoisture    502\n",
       "pH              410\n",
       "Fertilizer      479\n",
       "Location          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is one of the issues that the dataset curerntly has in terms of data quality.\n",
    "#Specifically it has multiple cells which have null values.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae33fc99-6ff6-4d64-ab44-10f1d0b0c107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s16', 's36', 's59', 'S009', 'S051', 'S17', 's32', 's48', 'S39',\n",
       "       'S027', 's14', 'S23', 's51', 'S21', 'S043', 'S52', 'S022', 'S049',\n",
       "       'S45', 's27', 's44', 's54', 'S032', 's49', 's10', 's29', 's30',\n",
       "       'S014', 'S14', 'S49', 'S59', 'S020', 'S011', 's42', 'S30', 's35',\n",
       "       's46', 's22', 'S046', 'S018', 's7', 's56', 'S016', 'S24', 's2',\n",
       "       'S054', 's08', 's33', 'S050', 'S46', 's26', 'S052', 's11', 's15',\n",
       "       's50', 's19', 'S2', 'S22', 'S004', 'S006', 'S31', 'S58', 'S025',\n",
       "       's03', 'S059', 'S20', 's34', 's23', 'S031', 'S055', 's43', 's25',\n",
       "       's41', 'S11', 's04', 's31', 'S023', 's40', 's38', 'S27', 's45',\n",
       "       's28', 'S034', 's12', 's55', 'S55', 'S34', 's24', 'S048', 's17',\n",
       "       'S53', 'S28', 'S033', 'S5', 'S51', 's01', 'S030', 'S9', 'S25',\n",
       "       'S045', 'S028', 'S36', 's6', 'S7', 'S029', 'S57', 's37', 'S33',\n",
       "       'S26', 'S036', 'S16', 'S3', 'S1', 'S003', 'S058', 'S6', 'S44',\n",
       "       's18', 's53', 'S35', 'S057', 'S041', 'S026', 's20', 's06', 's21',\n",
       "       'S035', 'S007', 'S008', 's5', 'S42', 's60', 'S060', 'S015', 'S19',\n",
       "       's9', 'S8', 'S002', 's4', 'S021', 's07', 'S047', 'S056', 'S54',\n",
       "       'S053', 's1', 'S29', 's8', 'S12', 'S044', 's09', 'S010', 'S019',\n",
       "       'S60', 's58', 'S38', 'S32', 'S038', 'S48', 's05', 's57', 'S037',\n",
       "       'S4', 'S43', 'S50', 'S039', 'S005', 's47', 's52', 's13', 'S18',\n",
       "       'S042', 's39', 'S001', 'S47', 'S40', 'S013', 'S012', 'S024', 'S37',\n",
       "       'S13', 's02', 'S017', 'S10', 'S56', 'S15', 'S41', 'S040', 's3'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Inconsistency is also present within the values in the dataset.\n",
    "#In the case below the sensor ids have different formats in that ids do not have \n",
    "#similar formats of things such as the s character either being capitalized \"S\" or \n",
    "#lowercased \"s\". Other inconsistencies that are present are that numbers may or may\n",
    "#not have leading zeroes.\n",
    "df['SensorID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1139d19-63ba-4b33-8795-a67fda3c35ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['12-Mar-25', '4/1/2025', '20-Mar-25', '3/31/2025', '9/4/2025',\n",
       "       '6/3/2025', '3/4/2025', '4/3/2025', '6-Apr-25', '15/03/2025',\n",
       "       '18/03/2025', '3/19/2025', '4/8/2025', '3/15/2025', '3/25/2025',\n",
       "       '3/3/2025', '4/4/2025', '2/25/2025', '3/23/2025', '10/3/2025',\n",
       "       '30/03/2025', '23-Mar-25', '7/4/2025', '4/6/2025', '25-Mar-25',\n",
       "       '28/03/2025', '8/3/2025', '3/9/2025', '5/3/2025', '3/24/2025',\n",
       "       '3-Apr-25', '2/26/2025', '14/03/2025', '4/9/2025', '10/4/2025',\n",
       "       '17-Mar-25', '1-Mar-25', '3/2/2025', '2/28/2025', '3/14/2025',\n",
       "       '28-Feb-25', '3/28/2025', '3/27/2025', '17/03/2025', '28/02/2025',\n",
       "       '3/20/2025', '4/7/2025', '6-Mar-25', '3/22/2025', '7-Apr-25',\n",
       "       '13-Mar-25', '2/3/2025', '31/03/2025', '3/21/2025', '5-Apr-25',\n",
       "       '27-Mar-25', '3/5/2025', '3/18/2025', '3/8/2025', '3/30/2025',\n",
       "       '29/03/2025', '4-Mar-25', '3/26/2025', '24-Mar-25', '3/29/2025',\n",
       "       '3/11/2025', '3/7/2025', '10-Mar-25', '3/12/2025', '5/4/2025',\n",
       "       '8-Mar-25', '1/3/2025', '8/4/2025', '4/2/2025', '4/5/2025',\n",
       "       '2-Apr-25', '3/1/2025', '4/10/2025', '30-Mar-25', '27-Feb-25',\n",
       "       '3/10/2025', '3/16/2025', '20/03/2025', '7/3/2025', '25/02/2025',\n",
       "       '3/17/2025', '26/02/2025', '6/4/2025', '2/27/2025', '3/13/2025',\n",
       "       '16-Mar-25', '16/03/2025', '3/6/2025', '15-Mar-25', '12/3/2025',\n",
       "       '26-Feb-25', '26-Mar-25', '1/4/2025', '9-Mar-25', '25/03/2025',\n",
       "       '4-Apr-25', '7-Mar-25', '9/3/2025', '10-Apr-25', '2/4/2025',\n",
       "       '27/02/2025', '23/03/2025', '9-Apr-25', '27/03/2025', '24/03/2025',\n",
       "       '25-Feb-25', '18-Mar-25', '13/03/2025', '19/03/2025', '2-Mar-25',\n",
       "       '11/3/2025', '31-Mar-25', '3-Mar-25', '22/03/2025', '29-Mar-25',\n",
       "       '1-Apr-25', '22-Mar-25', '11-Mar-25', '5-Mar-25', '21/03/2025',\n",
       "       '19-Mar-25', '8-Apr-25', '26/03/2025', '21-Mar-25', '28-Mar-25',\n",
       "       '14-Mar-25'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inconsistencies with other columns are also present. In the case of the date column\n",
    "#there are different formats of dates such as the following: 12-Mar-25, 28/03/2025, 3/31/2025\n",
    "df['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473dc366-41f7-4ee4-b52e-aa7e10907b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['15.9', '25', '32', '14', '17.1', nan, '22.2', '22.4', '33.1',\n",
       "       '16.4', '13.8', '25.4', '35.5', '26.1', '24', '30.4', '35.7',\n",
       "       '26.5', '34', '21.3', '22.8', '34.4', '21.4', '36.6', '14.7', '17',\n",
       "       '26.9', '19.1', '17.6', '33', '31.6', '18.7', '33.3', '23.2',\n",
       "       '30.8', '23.6', '18.8', '28.9', '17.2', '36.3', '16.7', '30.1',\n",
       "       '26.8', '22.1', '32.7', '25.7', '29.8', '16.8', '35.3', '28.1',\n",
       "       '20.9', '22.5', '17.4', '36', '24.2', '23.8', '32.1', '28.8',\n",
       "       '34.7', '33.9', '37', '20', '29.4', '24.7', '13.3', 'missing',\n",
       "       '33.6', '15.6', '21.8', '31.2', '30.9', '30.5', '13.4', '34.3',\n",
       "       '34.8', '18.6', '12.2', '28.3', '26.4', '37.9', '13.1', '28',\n",
       "       '27.6', '31.4', '36.1', '-30.9', '21', '29.2', '24.1', '27.5',\n",
       "       '23.7', '12.1', '35.8', '29.7', '27.4', '14.4', '20.1', '27.9',\n",
       "       '27', '15.5', '12.7', '23.4', '20.3', '13.6', '32.4', '29.9',\n",
       "       '385.2', '31.7', '28.6', '19', '15.7', '22', '34.5', '16', '28.5',\n",
       "       '14.1', '19.8', '25.6', '27.8', '17.8', '22.7', '21.5', '37.5',\n",
       "       '12.6', '15.8', '32.2', '23.3', '30', '26.7', '18.2', '22.3',\n",
       "       '27.1', '23.9', '19.6', '35.2', '25.1', '12.4', '13.7', '19.2',\n",
       "       '33.4', '20.2', '14.3', '25.5', '31.9', '29.1', '15', '32.9', '31',\n",
       "       '20.4', '33.2', '27.7', '33.5', '30.2', '26', '25.8', '37.1',\n",
       "       '34.2', '18', '19.3', '-91.4', '28.2', '29.6', '29.5', '17.3',\n",
       "       '23', '-78.7', '22.9', '19.4', '34.6', '14.2', '25.9', '19.7',\n",
       "       '-99.5', '35.9', '13.9', '19.9', '32.3', '23.5', '794.7', '24.4',\n",
       "       '37.6', '34.1', '24.6', '30.7', '33.8', '24.8', '28.4', '21.2',\n",
       "       '35.4', '25.2', '37.7', '17.5', '28.7', '-79.6', '12.9', '16.3',\n",
       "       '-86.2', '17.9', '21.6', '16.6', '37.2', '37.8', '36.5', '36.7',\n",
       "       '21.9', '-43.5', '13.2', '35', '23.1', '14.9', '35.1', '14.5',\n",
       "       '37.3', '32.8', '32.5', '29', '29.3', '15.3', '36.8', '-65.5',\n",
       "       '20.6', '14.6', '-3.2', '20.7', '31.5', '478.8', '20.8', '12',\n",
       "       '38', '-66.1', '13.5', '17.7', '14.8', '22.6', '32.6', '21.7',\n",
       "       '15.1', '26.2', '15.2', '16.1', '16.5', '31.8', '12.3', '16.2',\n",
       "       '13', '36.4', '573.7', '36.2', '30.6', '18.4', '37.4', '30.3',\n",
       "       '19.5', '24.3', '-81.3', '12.5', '15.4', '-36.5', '26.6', '672.3',\n",
       "       '34.9', '31.1', '36.9', '18.1', '20.5', '-98.1', '31.3', '24.5',\n",
       "       '18.9', '214.3', '16.9', '-54.5', '-16.2', '77.6', '383.8', '-5.9',\n",
       "       '663.6', '-30.6', '21.1', '184.3', '18.3', '-52.9', '-63.6',\n",
       "       '173.7', '24.9', '33.7', '27.3', '26.3', '115.3', '-99.8', '25.3',\n",
       "       '18.5', '416.7', '460.6', '35.6', '-52', '579.1', '-30', '-19.6',\n",
       "       '-82.6', '631.3', '27.2', '12.8', '616.9', '552.6', '-87.5',\n",
       "       '191.5', '-22', '227.4', '-42.4', '-24.6', '-38.2', '65.6',\n",
       "       '563.3', '267.1', '422.7', '-71', '-58.9', '685.4', '-55.8',\n",
       "       '120.1', '-2.8', '-86.7', '-44', '-35.9', '-60.8', '-13.7',\n",
       "       '625.6', '-66.5', '362.4', '-8.1', '-69.9', '443.8', '-26.5',\n",
       "       '754.2', '497.7', '-22.3', '741', '191.6', '-70.2', '568', '-39',\n",
       "       '-14.9', '790.5', '100', '364', '-13.9', '122.1', '-64.9', '-98.3',\n",
       "       '-74', '401', '-82.5', '175.1', '686.7', '331.8', '247.2', '-54.1',\n",
       "       '744.6', '-91.5', '247.4', '435.3', '-3.8', '595.9', '-14.3',\n",
       "       '397.8', '-95.1', '-79.7', '-9.8', '73.6', '-73.5', '-28.2',\n",
       "       '-73.4', '494.2', '235.9', '630', '-32.9', '320.2', '693.7',\n",
       "       '578.8', '-86.4', '-97.8', '-37.9', '-59.1', '384.2', '-68.7',\n",
       "       '-34.1', '773.2', '-47.2', '201.7', '-91', '794.9', '-63.3',\n",
       "       '466.2', '-7.4', '-54.3', '267', '133.3', '175', '440.5', '132.4',\n",
       "       '538.6', '743.6', '205.2', '71.2', '-67.4', '-16.6', '-94.8',\n",
       "       '547', '626.2', '257.4', '687.7', '-35.5', '188.5', '130.6',\n",
       "       '659.6', '-53.8', '522.7', '-52.4', '483.7', '96.3', '-78.5',\n",
       "       '-85.6', '-76.7', '-39.8', '283.6', '-3.9', '-96.2', '-10.9',\n",
       "       '657.2', '-68.1', '-21.4', '183', '376.5', '-83.9', '-25.3', '455',\n",
       "       '-10.1', '750', '-85.3', '687.1', '770.6', '-54.8', '278.6',\n",
       "       '796.5', '-77.9', '147', '-37.2', '-67.2', '719.5', '-33.8', '-79',\n",
       "       '-67.5', '-55.6', '-61.4', '-86.9', '-5.3', '490.8', '759.3',\n",
       "       '519.4', '-60', '-89', '-46', '-23.5', '200.2', '-69.2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inconsistencies with other columns are also present. Such as in the case of the temperatue column\n",
    "#values are not consistent in that there are some strings such as nan, or missing that are present.\n",
    "\n",
    "#Issues with data types are also presnet in that integers and floats are present with no clear format to follow.\n",
    "df['Temperature'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fa253a-d2f7-46f2-a59e-af58b62a22e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['25', '50', '54', '55', '56', '62', '27', '26', '42', '24', '63',\n",
       "       '19', '64', '22', '33', '41', nan, '58', '18', '38', '57', '39',\n",
       "       '59', '20', '43', 'missing', '30', '48', '28', '9999', '51', '44',\n",
       "       '47', '37', '35', '34', '45', '60', '53', '21', '52', '65', '23',\n",
       "       '29', '61', '49', '40', '31', '32', '36', '46'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inconsistencies with other columns are also present. Such as in the case of the temperatue column\n",
    "#values are not consistent in that there are some strings such as nan, or missing are present.\n",
    "df['SoilMoisture'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3819a738-172f-4054-9396-24cf35d1add8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1.4', '1.7', '1.6', '3.7', '4.2', '1.2', '2.8', '3.4', '2.5',\n",
       "       '3.3', '1.8', '1.3', '3', '3.1', '2.7', '3.8', '3.2', '3.6', '2.1',\n",
       "       nan, '0.006', '2.9', '4', '2.6', '3.9', '1.9', '2.3', '2.4', '3.5',\n",
       "       'missing', '1.5', '4.1', '2', '2.2', '1994.2', '3340.1', '3063.1',\n",
       "       '2058.3', '3769.4', '2281.8', '1318.1', '0.008', '2049.6',\n",
       "       '3911.2', '2541.6', '0.005', '0.003', '3765', '2044.3', '0.007',\n",
       "       '2450.1', '0.004', '1090', '1543.1', '3915', '3263.8', '3071.8',\n",
       "       '0.001', '3729.4', '3992', '2749.1', '2503', '3315.9', '2288.3',\n",
       "       '3122', '3568.5', '3275.5', '1902.3', '1759.3', '2221.5', '3302.7',\n",
       "       '1366', '2030.1', '1109.8', '2426.7', '0.002', '2407.3', '3458.4',\n",
       "       '3525', '1092.9', '3565.2', '1411.2', '1114.2', '1736.7', '2353.3',\n",
       "       '2366', '3027.3', '2745.1', '1339.2', '3549.2', '1036.2', '3014.6',\n",
       "       '1304.9', '1585.6', '2948.8', '3219', '3084.7', '1308', '0.009',\n",
       "       '1749.2', '1611.7', '1428.1', '3135.7', '1646.2', '3156.7',\n",
       "       '3918.6', '1690.7', '3449.5', '3690.5', '1036.4', '3786.1',\n",
       "       '2401.6', '3593.6', '1775.7', '3345.8', '2698.4', '2184', '3935.8',\n",
       "       '2119.7', '3737.6', '1147.7', '3876.7', '2996.4', '3765.5',\n",
       "       '3608.9', '1778.9', '1886', '2117.6', '1657.4', '3144.2', '3733.8',\n",
       "       '3575.8', '3284.9', '2489.2', '2190.5', '2748', '2673.3', '1319.8',\n",
       "       '3830.5', '3431', '2880.2', '2835.3', '3867.2', '2550.7', '1103.7',\n",
       "       '1252.2', '1743'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inconsistencies with other columns are also present. Such as in the case of the temperatue column\n",
    "#values are not consistent in that there are values that use the kg and g format respectively.\n",
    "df['Fertilizer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3832bb0-3c8f-4013-83d5-221e6717825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['n field', 'South Field', 'EASTFIELD', 'West Field', 'W field',\n",
       "       'North Field', 'NorthField', 'SOUTH FIELD', 'Southfield',\n",
       "       'NORTH FIELD', 'w field', 'East Field', 'Nrth Field',\n",
       "       'north field', 'S field', 'westField', 'EastField', 'e field',\n",
       "       'WEST FIELD', 'east field'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inconsistencies with other columns are also present. There are multiple variations of formats\n",
    "#that pertains to the location such as in the case of northfield mutliple formats are present such\n",
    "#as n field, NorthField, NORTH FIELD, North Field\n",
    "df['Location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d5d9c3-5138-44a7-a61a-2ea7cfa21b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['7.8', '6.9', '5.9', '7.4', '6.8', '7', '6.3', '6.6', '7.6', '6.7',\n",
       "       '6.1', '7.3', '5.6', '7.2', '5.8', '6.4', nan, '6.2', '6.5', '7.7',\n",
       "       '6', '-1.2', '5.7', '7.5', 'missing', '5.5', '7.1', '-0.9', '14',\n",
       "       '1', '0.6', '1.1', '9.5', '9.2', '14.3', '12.2', '9.1', '0.3',\n",
       "       '13.1', '9.9', '2.1', '-1', '12.3', '1.5', '2.3', '13.2', '0.7',\n",
       "       '10.4', '-1.6', '2.2', '0.8', '10.2', '1.9', '9.3', '13.5', '14.7',\n",
       "       '14.4', '0.4', '-0.3', '13.9', '0.2', '12.4', '14.8', '0.9',\n",
       "       '12.7', '11.2', '-1.3', '10.9', '14.9', '1.2', '-0.8', '-1.7',\n",
       "       '14.6', '2.5', '0.5', '9.4', '11', '-1.8', '12.5', '1.4', '10.6',\n",
       "       '-1.9', '2.4', '2', '-0.6', '11.6', '1.7', '13.7', '14.5', '10',\n",
       "       '11.1', '9.8', '10.1', '12.1', '-1.5', '0.1', '11.5', '12.8',\n",
       "       '11.9', '-0.2', '13.8', '-0.1', '14.1', '10.8', '13.4', '-1.1',\n",
       "       '-0.7', '10.7', '11.4', '-2', '12', '1.3', '10.3', '-0.4', '-1.4',\n",
       "       '11.7', '9.7', '-0.5', '11.3', '11.8', '1.8', '0', '14.2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inconsistencies with other columns are also present. Such as in the case of the temperatue column\n",
    "#values are not consistent in that there are some strings such as nan, or missing are present.\n",
    "\n",
    "#Noisy values are also present within the dataset, as negative ph values are present wherein the normal ph scale\n",
    "#only considers values from 0-14\n",
    "df['pH'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485fbcb8-3f5d-497b-a346-359955d03fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(197)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The code below shows the number of duplicate rows that are present within the dataset\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb0301-fec6-4ba8-a49d-1c8b20e2030a",
   "metadata": {},
   "source": [
    "## 2. Given the variety of dirty values (N/A, None, blank, \"missing\", etc.), write a Python code snippet to identify all rows where any value is missing, according to the lecture’s definitions of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81057de8-ab64-4906-8839-3d63773b77c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SensorID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>SoilMoisture</th>\n",
       "      <th>pH</th>\n",
       "      <th>Fertilizer</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S17</td>\n",
       "      <td>6/3/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>North Field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S022</td>\n",
       "      <td>3/25/2025</td>\n",
       "      <td>35.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>w field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>s10</td>\n",
       "      <td>23-Mar-25</td>\n",
       "      <td>36.6</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.3</td>\n",
       "      <td>North Field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>S020</td>\n",
       "      <td>3/19/2025</td>\n",
       "      <td>31.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>westField</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>S011</td>\n",
       "      <td>5/3/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.006</td>\n",
       "      <td>West Field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10677</th>\n",
       "      <td>s31</td>\n",
       "      <td>3/23/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>east field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10681</th>\n",
       "      <td>s52</td>\n",
       "      <td>3/9/2025</td>\n",
       "      <td>35.2</td>\n",
       "      <td>46</td>\n",
       "      <td>5.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EastField</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10690</th>\n",
       "      <td>s4</td>\n",
       "      <td>3/5/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>w field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10696</th>\n",
       "      <td>s21</td>\n",
       "      <td>3/20/2025</td>\n",
       "      <td>12.4</td>\n",
       "      <td>19</td>\n",
       "      <td>7.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EastField</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10703</th>\n",
       "      <td>S33</td>\n",
       "      <td>4/9/2025</td>\n",
       "      <td>37.4</td>\n",
       "      <td>29</td>\n",
       "      <td>7.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOUTH FIELD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2166 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SensorID       Date Temperature SoilMoisture   pH Fertilizer  \\\n",
       "5          S17   6/3/2025         NaN           62  6.8        1.2   \n",
       "16        S022  3/25/2025        35.7          NaN  5.9        1.3   \n",
       "25         s10  23-Mar-25        36.6           43  NaN        3.3   \n",
       "33        S020  3/19/2025        31.6          NaN  7.4        NaN   \n",
       "34        S011   5/3/2025         NaN           30  7.7      0.006   \n",
       "...        ...        ...         ...          ...  ...        ...   \n",
       "10677      s31  3/23/2025         NaN           28  NaN          4   \n",
       "10681      s52   3/9/2025        35.2           46  5.9        NaN   \n",
       "10690       s4   3/5/2025         NaN           45  6.6        2.5   \n",
       "10696      s21  3/20/2025        12.4           19  7.5        NaN   \n",
       "10703      S33   4/9/2025        37.4           29  7.5        NaN   \n",
       "\n",
       "          Location  \n",
       "5      North Field  \n",
       "16         w field  \n",
       "25     North Field  \n",
       "33       westField  \n",
       "34      West Field  \n",
       "...            ...  \n",
       "10677   east field  \n",
       "10681    EastField  \n",
       "10690      w field  \n",
       "10696    EastField  \n",
       "10703  SOUTH FIELD  \n",
       "\n",
       "[2166 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define key words that determine dirty values within the dataset\n",
    "dirty_values = [\"\", \"missing\", \"n/a\", \"na\", \"null\"]\n",
    "\n",
    "#Convert the defined dirty values above into nan\n",
    "df = df.map(lambda x: np.nan if isinstance(x, str) and x.strip().lower() in dirty_values else x)\n",
    "\n",
    "#Find all the rows where the nan value exists and store them all into the missing_rows dataframe\n",
    "missing_rows = df[df.isnull().any(axis=1)]\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70dd446-02c1-4832-b8c6-43485878dc51",
   "metadata": {},
   "source": [
    "## 3. Using principles from the discussion, propose a specific method to impute missing values for the \"SoilMoisture\" attribute. Justify your choice based on the characteristics of this data and the pros/cons of methods from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b470b278-b796-40d6-b9ef-93c96ca909ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median value to replace missing values: 42.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The code below replaces any missing values for soil moisture based on the median of value of the range \n",
    "#of values present within the column, instead of using mean in our case this was done to ensure that outliers\n",
    "#that are present within the unclean dataset cannot skew the value that we would be replacing the missing values with.\n",
    "\n",
    "df['SoilMoisture'] = pd.to_numeric(df['SoilMoisture'], errors='coerce')\n",
    "df['SoilMoisture'] = df['SoilMoisture'].fillna(df['SoilMoisture'].median())\n",
    "print(f\"Median value to replace missing values: {df['SoilMoisture'].median()}\")\n",
    "df['SoilMoisture'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b9882-c552-4fc3-9999-11dc6ad971fd",
   "metadata": {},
   "source": [
    "## 4. Describe, with pseudocode or code, how you would detect and remove outliers in the \"Temperature\" column. Be explicit about the criteria you would use, referencing binning or statistical approaches in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "719343a2-3205-4f33-892e-e434ffe9c554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108      -30.9\n",
       "140      385.2\n",
       "242      -91.4\n",
       "258      -78.7\n",
       "278      -99.5\n",
       "         ...  \n",
       "10386    -89.0\n",
       "10450    -46.0\n",
       "10501    -23.5\n",
       "10590    200.2\n",
       "10663    -69.2\n",
       "Name: Temperature, Length: 211, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For detecting outliers we use the Interquartile Range (IQR) method\n",
    "#wherein we use the Q1 and Q3 values to derive a range of values that we consider within\n",
    "#the normal range, and exclude any values outside of that range.\n",
    "df['Temperature'] = pd.to_numeric(df['Temperature'], errors='coerce')\n",
    "\n",
    "Quartile1 = df['Temperature'].quantile(0.25)\n",
    "Quartile3 = df['Temperature'].quantile(0.75)\n",
    "IQR = Quartile3  - Quartile1\n",
    "\n",
    "lower_threshold = Quartile1- 1.5 * IQR\n",
    "upper_threshold = Quartile3 + 1.5 * IQR\n",
    "\n",
    "df_cleaned = df[(df['Temperature'] >= lower_threshold) & (df['Temperature'] <= upper_threshold)]\n",
    "\n",
    "outliers = df[(df['Temperature'] < lower_threshold) | (df['Temperature'] > upper_threshold)]\n",
    "outliers['Temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23017ac6-4e88-432a-8831-05e1ebab82bd",
   "metadata": {},
   "source": [
    "## 5. Explain a systematic approach (not a code) for correcting location name inconsistencies (e.g., \"NORTH FIELD\", \"n field\", \"North field\") using the slides’ recommendations for resolving inconsistencies and field overloading.\n",
    "\n",
    "- In properly correcting location name inconsistencies such as those related to north field, what we can firstly do is skim through all our data points and list out all the unique variations of north field, in our case we get the results NORTH FIELD, n field, and  North field. The next thing that I would do is determine how I would want to format these data entires into a unified style, in this case we would simple just convert every character into lowercase and convert them into the format north field. So what I would then do is iterate over all the instances of north field and we would apply a lowercase function for all words, and if we detect a n with whitespace, using a dictionary or any form of referencing method, we would replace that n value with north. In the end we would be resolving all of the inconsistencies that are assocaited with that speicified location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdcf8e5-edf6-486d-8cf5-a28f1324fe1e",
   "metadata": {},
   "source": [
    "## 6. Sensors have sometimes uploaded duplicate or near-duplicate rows (same sensor, time, location, but slight variation in other fields). Describe an algorithm or workflow (can include coding or database operations) to detect and remove these duplicates. Reference principles or tools discussed in the slides for data discrepancy detection and ETL\n",
    "\n",
    "- In handling the duplicate values pertaining to the sensors what I would personally do is firstly to handle the inconsistencies within the dataset mainly targeting columns such as SensorId, Date, and Location as this would be the main\n",
    "indicators for duplicate values. In handling sensor id what we would do is to mainly convert the 's' and 'S' into a specific format, such as in our case we will convert it all into lowercase 's', next thing to do is to handle and remove the leading zeroes after the 's' character, this would result in formatting all of the SensorID into a unified format. The next column that we would need to handle is the date format, in this case we would convert the different date formats present which are these 12-Mar-25, 28/03/2025, 3/31/2025 into a unified format such as mm/dd/yy, we could use regex patterns to properly identify each of the different cases and apply the necessary formatting for each to conclude into our unified mm/dd/yy format. The last column that we would need to handle is the Location, wherein we would need to handle all of the inconsistencies for all the Locations, we have to consider formats such as \"NORTH FIELD\", \"n field\", \"North field\" and considering other locations such as east, west, and south. In doing so, like from what I have stated preivously, we could just convert all characters into a specified casing, such as lowercase, and convert isolated characters such as n into their corresponding location using things such as dictionaries. After these have been done we could simply use methods present within python libraries such as the df.drop_duplicates method to drop duplicate records based on these three columns of sensor, date, and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06840cb1-ab2c-4e13-b6fd-d800b2860065",
   "metadata": {},
   "source": [
    "## 7. The dataset has wildly varying date formats. Describe a robust strategy or Python approach (not just pd.to_datetime) to standardize dates, including how to handle ambiguous or failed parsing, inspired by lecture content on data transformation\n",
    "\n",
    "- The code below shows a detailed strategy on how to handle standardizing date time format.\n",
    "What would mainly happen is we should firstly consider missing or null values and assign a NaT value\n",
    "to them if detected. Now we would process and determine the format of each of the date formats and\n",
    "this may be done by means of using regex formulas, as this would consistently and exactly match known\n",
    "formats for processing. What would then happen is that we would convert these found formats for each case\n",
    "into a unified structure such as mm/dd/yy. Additionally we should also not purely rely on just using the pd.to_datetime\n",
    "as using this without considering the formats may end up in improperly identifying the dates an example of this would be\n",
    "in the case of the date '12/03/2025' or '3/31/2025', this should be interpreted into two ways but if we just apply that method to both of this without considering the format it may obstruct formats that the method may not deem valid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b877e464-e2d7-47e3-9ec5-61dd3fb476b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['03/12/25', '04/01/25', '03/20/25', '03/31/25', '09/04/25',\n",
       "       '06/03/25', '03/04/25', '04/03/25', '04/06/25', '03/15/25',\n",
       "       '03/18/25', '03/19/25', '04/08/25', '03/25/25', '03/03/25',\n",
       "       '04/04/25', '02/25/25', '03/23/25', '10/03/25', '03/30/25',\n",
       "       '07/04/25', '03/28/25', '08/03/25', '03/09/25', '05/03/25',\n",
       "       '03/24/25', '02/26/25', '03/14/25', '04/09/25', '10/04/25',\n",
       "       '03/17/25', '03/01/25', '03/02/25', '02/28/25', '03/27/25',\n",
       "       '04/07/25', '03/06/25', '03/22/25', '03/13/25', '02/03/25',\n",
       "       '03/21/25', '04/05/25', '03/05/25', '03/08/25', '03/29/25',\n",
       "       '03/26/25', '03/11/25', '03/07/25', '03/10/25', '05/04/25',\n",
       "       '01/03/25', '08/04/25', '04/02/25', '04/10/25', '02/27/25',\n",
       "       '03/16/25', '07/03/25', '06/04/25', '12/03/25', '01/04/25',\n",
       "       '09/03/25', '02/04/25', '11/03/25'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The code below demonstrates how we could standardize dates.\n",
    "import re\n",
    "\n",
    "def format_date(val):\n",
    "    if pd.isna(val):\n",
    "        return pd.NaT\n",
    "\n",
    "    val = str(val).strip()\n",
    "\n",
    "    # Accepts the Format: '12-Mar-25' (DD-MMM-YY)\n",
    "    if re.fullmatch(r'\\d{1,2}-[A-Za-z]{3}-\\d{2}', val):\n",
    "        dt = pd.to_datetime(val, format='%d-%b-%y', errors='coerce')\n",
    "    # Accepts the Format: '12/03/2025' or '3/31/2025'\n",
    "    elif re.fullmatch(r'\\d{1,2}/\\d{1,2}/\\d{4}', val):\n",
    "        parts = list(map(int, val.split('/')))\n",
    "        day, month = parts[0], parts[1]\n",
    "        dt = pd.to_datetime(val, dayfirst=(day > 12), errors='coerce')\n",
    "    else:\n",
    "        dt = pd.to_datetime(val, errors='coerce')\n",
    "\n",
    "    # Return formatted string or NaT\n",
    "    return dt.strftime('%m/%d/%y') if not pd.isna(dt) else pd.NaT\n",
    "\n",
    "\n",
    "df['Date'] = df['Date'].apply(format_date)\n",
    "df['Date'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f94876-8bde-4df8-82a4-ee57d4204209",
   "metadata": {},
   "source": [
    "## 8. The \"Fertilizer\" column has values in both kg and g (some mistakenly entered as “0.003” or “3.2” interchangeably). Propose a practical data cleaning pipeline to standardize this column, referencing transformation and normalization from the discussion slides\n",
    "\n",
    "- A data cleaning process that we can apply to the Fertilzer column could be in the form of gaining a understanding of the dataset, in our case our data set contains two distinct classes of data which are in the form of data being under <10 and other values greatly exceeding >10 and usually hovering around the 1000+ range of values. What we could then proceeed to do is handle missing or inappropriate values by denoting them as nan, and if number is < 10 we would just retain the value and assume its in the format of kg, and then on other hand if its >10 we would convert the values into kg by dividing by 1000. This would then result in our column data being converted into kg format thus standardizing and transforming our data properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c8d65-7e61-4107-b5e7-bf4284d5c5e1",
   "metadata": {},
   "source": [
    "## 9. After all cleaning steps, outline two EDA (Exploratory Data Analysis) checks you would perform to confirm your data is clean and ready for mining, and explain why they are important, referencing the principles from the discussion summary.\n",
    "\n",
    "- One EDA process that I would perform on the dataset after I have applied the necessary cleaning and data processing is to generate graphs and plots that would allow me to verify outliers and other unexpected occurences within the dataset. This process may be important to verify if our cleaning steps was properly applied to our dataset, and the steps we did are valid in that they did not tamper with the data in a way that would be detrimental to understanding the dataset as a whole.\n",
    "\n",
    "- Another EDA process that I would perform on the dataset relies on using methods such as unique, isnull, duplicated, and value_counts to determine if the values within the dataset are all in proper order, and that the other cleaning methods that we used properly fixed our data to the point that it can be used for model creation and interpretation without involving outliers and other mismatch and inconsistencies between data points. This is important to do as a final check, as these methods would easily allow us to validate our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d31ebc-0e20-404e-b887-a811ad00d2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
